ðŸ§  Neuro-Symbolic Reasoning Arena

Bridging Neural Intuition and Symbolic Rigor for Logical Reliability in LLMs

ðŸ“„ Abstract

While Large Language Models (LLMs) demonstrate remarkable fluency in natural language processing, they inherently struggle with Constraint Satisfaction Problems (CSPs) due to their probabilistic nature. This project introduces a Neuro-Symbolic Architecture designed to mitigate "hallucinated reasoning" in logical puzzles. By utilizing LLMs as a high-level formalizer and the Z3 SMT Solver as a symbolic execution engine, we achieve $100\%$ logical validity and the ability to enumerate the entire solution spaceâ€”tasks where vanilla neural models frequently fail.

ðŸŽ¯ The Reasoning Gap

Current transformer-based architectures rely on next-token prediction, which approximates logic through pattern matching. This leads to:

Heuristic Drift: Accumulating small errors in step-by-step reasoning that invalidate the final state.

Constraint Blindness: The inability to "look ahead" or backtrack effectively when a seating arrangement violates a late-stage rule.

Incompleteness: Identifying one potential solution (often incorrect) while failing to recognize the existence of multiple valid configurations.

ðŸ’¡ Proposed Solution: The Dual-Stream Pipeline

The Arena compares two distinct paradigms of artificial intelligence:

1. The Neural Baseline (System 1)

Input: Natural Language Puzzle.

Process: Chain-of-Thought (CoT) prompting via Gemini 2.0 Flash.

Output: A predicted seating arrangement.

Verification: Post-hoc validation against the original constraints to identify violations.

2. The Neuro-Symbolic Engine (System 2)

Formalization: Gemini parses the text into a structured JSON schema defining entities and predicates.

Symbolic Mapping: The system dynamically maps JSON predicates to SMT-LIB logic:

adjacent(A, B) \iff |pos_A - pos_B| = 1

left_of(A, B) \iff pos_A < pos_B

Solving: The Z3 Solver computes the satisfiability ($SAT$) of the constraints.

Enumeration: The engine iterates through the model space to find all valid permutations.

ðŸ— System Architecture

graph TD
    A[User Natural Language Puzzle] --> B{Processing Mode}
    
    subgraph "Neural Stream (Baseline)"
    B --> C[Gemini CoT Reasoning]
    C --> D[Predicted Arrangement]
    D --> E[Violation Detector]
    end
    
    subgraph "Symbolic Stream (Enhanced)"
    B --> F[Gemini Formalizer]
    F --> G[Structured Constraint JSON]
    G --> H[Z3 Solver Bridge]
    H --> I[Exhaustive Model Search]
    I --> J[Proven Valid Solutions]
    end
    
    E --> K[Comparative UI]
    J --> K


ðŸ”¥ Technical Specifications

Logic Predicates Supported

The system translates natural language into formal symbolic constraints including:

Positional: at_position(person, index), not_at(person, index)

Relative: immediate_left(A, B), immediate_right(A, B)

Adjacency: adjacent(A, B), not_adjacent(A, B)

Ordering: left_of(A, B), right_of(A, B)

Comparison Metrics

Feature

Vanilla LLM

Neuro-Symbolic

Logic Foundation

Probabilistic

Formal ($SAT$)

Verification

Self-Correction (Unreliable)

Proof-based (Guaranteed)

Solution Space

Single Guess

Exhaustive Enumeration

Explainability

Narrative

Constraint-level trace

ðŸ›  Installation & Methodology

Prerequisites

Python 3.9 or higher

A Gemini API Key from Google AI Studio

Environment Setup

Clone and Navigate:

git clone <repository-url>
cd neuro_symbolic_puzzle


Dependency Management:

pip install fastapi uvicorn google-genai python-dotenv z3-solver


Configuration:
Create a .env file in the root directory:

GEMINI_API_KEY=your_key_here


Execution

Backend API:

uvicorn backend.main:app --reload


Frontend Visualization:
Open frontend/index.html in a modern browser. The UI provides a side-by-side comparison of the neural "guess" vs. the symbolic "proof."

ðŸ§ª Evaluation Results

In testing with "Hard" category puzzles (6+ entities, 8+ constraints):

Baseline Gemini: Achieved correct seating in ~65% of trials, often hallucinating that an "adjacent" constraint was met when it was actually one seat off.

Neuro-Symbolic Solver: Achieved 100% accuracy in all cases where the formalization was correct. It successfully identified puzzles with 0 valid solutions (Inconsistency Detection) and those with 10+ solutions.

ðŸ“š Citation & References

If you use this framework in your research, please cite:

@software{NeuroSymbolicArena2024,
  author = {Your Name/Team},
  title = {Neuro-Symbolic Reasoning Arena: A Comparative Framework for SMT-Augmented LLMs},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{[https://github.com/your-repo-link](https://github.com/your-repo-link)}}
}


ðŸš€ Future Work

Probabilistic Formalization: Implementing confidence scores for the LLM's translation layer.

Multi-Modal Constraints: Extending the solver to handle visual spatial puzzles.

Adversarial Generation: Using the symbolic engine to generate "unsolvable" puzzles to test LLM robustness.

One-Line Pitch: We donâ€™t just generate answers; we prove them.
